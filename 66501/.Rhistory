}))
cities.df.new = data.frame(
place=as.character(transpose.apply.df[,1]),
lat=as.numeric(transpose.apply.df[,2]),
lon=as.numeric(transpose.apply.df[,3]))
cities.df.new$place = as.character(cities.df.new$place)
validLocs$lat = cities.df.new$lat
validLocs$lon = cities.df.new$lon
world = c(left=-170,
bottom=-70,
right=170,
top=70)
map = get_stamenmap(world,
zoom = 2,
maptype = "toner-lite")
validLocs.avg.followers = validLocs %>%
group_by(lat,lon) %>%
summarize(avg_followers=mean(user_followers))
validLocs$hashTagList = as.character(validLocs$hashtags)
validLocs$tagList = apply(validLocs,1,
function(row){
lstStr = row[["hashTagList"]]
items = gsub(" ", "",
strsplit(substr(lstStr,
2,
nchar(lstStr)-1),
","))[[1]]
return(tolower(items))
})
allTags = unlist(validLocs$tagList)
top8Tags = names(head(sort(table(allTags[allTags != "character(0)"]),
decreasing = T),8))
top8Tags
subsetRows = filter(validLocs, tagList %in% top8Tags)
ggmap(map) +
geom_point(data=subsetRows,
aes(x=lon,
y=lat,
color=tagList),
alpha=0.3) +
labs(title="World Map of Twitter Users",
subtitle="Color Denoting Hash Tag in Tweet",
y="Latitude",
x="Longitude",
col="Hash Tags")
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
data = read_csv("https://raw.githubusercontent.com/zjbranson/315Spring2021/master/redditData.csv")
library(tm)
preProcess = function(col) {
col = Corpus(VectorSource(col))
col = tm_map(col, content_transformer(tolower))
col = tm_map(col, removePunctuation)
col = tm_map(col, stripWhitespace)
return(col)
}
title = preProcess(data$title)
DocumentTermMatrix(title,
control =
list(stopwords = FALSE,
stemming = FALSE))
dtm.title = DocumentTermMatrix(title,
control =
list(stopwords = T,
stemming = T))
dtm.title
colSums(as.matrix(dtm.title)) %>%
sort(.,decreasing=T) %>%
data.frame(count=.) %>%
top_n(10)
#male and female subsets
data.male = subset(data, gender == "M")
data.female = subset(data, gender == "F")
#redefine as Corpus
title.male = Corpus(VectorSource(data.male$title))
title.female = Corpus(VectorSource(data.female$title))
title.male = preProcess(data.male$title)
title.female = preProcess(data.female$title)
dtm.title.male = DocumentTermMatrix(title.male,
control =
list(stopwords = T,
stemming = T))
dtm.title.female = DocumentTermMatrix(title.female,
control =
list(stopwords = T,
stemming = T))
dtm.title.male
dtm.title.female
getTopTenWords = function(dtm) {
colSums(as.matrix(dtm)) %>%
sort(.,decreasing=T) %>%
data.frame(count=.) %>%
top_n(10)
}
getTopTenWords(dtm.title.male)
getTopTenWords(dtm.title.female)
wordFreqs = function(dtm) {
dtm.words = colnames(dtm)
dtm.freqs = colSums(as.matrix(dtm))
return(list(words=dtm.words,
freqs=dtm.freqs))
}
lstWordFreqs = wordFreqs(dtm.title)
wordcloud(words=lstWordFreqs$words,
freq=lstWordFreqs$freqs,
max.words=50,
random.order=F,
colors="darkblue",
rot.per=0.25)
par(mfrow=c(1,2))
lstWordFreqs.male = wordFreqs(dtm.title.male)
wordcloud(words=lstWordFreqs.male$words,
freq=lstWordFreqs.male$freqs,
max.words=50,
random.order=F,
colors="darkblue",
rot.per=0.25)
lstWordFreqs.female = wordFreqs(dtm.title.female)
wordcloud(words=lstWordFreqs.female$words,
freq=lstWordFreqs.female$freqs,
max.words=50,
random.order=F,
colors="darkgreen",
rot.per=0.25)
titleMaleFemale = tm:::c.VCorpus(title.male, title.female)
titleMaleFemale = tm_map(titleMaleFemale, PlainTextDocument)
#remove stop words, perform stemming
titleMaleFemale = tm_map(titleMaleFemale, removeWords, stopwords("english"))
titleMaleFemale = tm_map(titleMaleFemale, stemDocument)
#term-document matrix
tdm_maleFemale <- TermDocumentMatrix(titleMaleFemale,
control = list(
weighting = weightTfIdf))
#convert to a matrix class
tdm_maleFemale = as.matrix(tdm_maleFemale)
tdm_male_sum = rowSums(tdm_maleFemale[,1:5153])
tdm_female_sum = rowSums(tdm_maleFemale[,5154:10000])
tdm_maleFemale_sum = cbind(tdm_male_sum,
tdm_female_sum)
#rename the columns according to the group
colnames(tdm_maleFemale_sum) = c("Male" ,"Female")
comparison.cloud(tdm_maleFemale_sum,
random.order = FALSE,
colors = c("blue", "green"),
max.words = 100)
wordFreqs.male.titles = colSums(as.matrix(dtm.title.male)) %>%
sort(.,decreasing=T)
maleTitle.df = data.frame(word=names(wordFreqs.male.titles),
freq=as.numeric(wordFreqs.male.titles))
wordFreqs.female.titles = colSums(as.matrix(dtm.title.female)) %>%
sort(.,decreasing=T)
femaleTitle.df = data.frame(word=names(wordFreqs.female.titles),
freq=as.numeric(wordFreqs.female.titles))
library(tidytext)
library(tidyverse)
#label each word with a positive or negative sentiment
#(if it has one)
maleSentiments = maleTitle.df %>%
inner_join(get_sentiments("bing"))
femaleSentiments = femaleTitle.df %>%
inner_join(get_sentiments("bing"))
sentimentFreqsMale = maleSentiments %>%
group_by(sentiment) %>%
summarise(totalFreq=sum(freq))
sentimentFreqsFemale = femaleSentiments %>%
group_by(sentiment) %>%
summarise(totalFreq=sum(freq))
x=c(sentimentFreqsMale[2,]$totalFreq,
sentimentFreqsFemale[2,]$totalFreq)
n = c(apply(as.matrix(sentimentFreqsMale$totalFreq),2,sum),
apply(as.matrix(sentimentFreqsFemale$totalFreq),2,sum))
prop.test(x,n)
x
x = c(weekend.tweets.props[1,]$n,
workday.tweets.props[1,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)),
workday.tweets.props %>% summarise(totalFreq=sum(n)))
prop.test(x,n)
n
x = c(weekend.tweets.props[1,]$n,
workday.tweets.props[1,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
n
x = c(weekend.tweets.props[1,]$n,
workday.tweets.props[1,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
prop.test(x,n)
x = c(weekend.tweets.props[1,]$n,
workday.tweets.props[1,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
prop.test(x,n,
aalternative = "greater")
x = c(weekend.tweets.props[1,]$n,
workday.tweets.props[1,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
prop.test(x,n,
alternative = "greater")
x = c(weekend.tweets.props[1,]$n,
workday.tweets.props[1,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
prop.test(x,n,alternative = "greater")
x = c(weekend.tweets.props[2,]$n,
workday.tweets.props[2,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
prop.test(x,n,alternative = "greater")
x = c(weekend.tweets.props[2,]$n,
workday.tweets.props[2,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
prop.test(x,n,alternative = "less")
x = c(weekend.tweets.props[1,]$n,
workday.tweets.props[1,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
prop.test(x,n,alternative = "greater")
workday.tweets.props = covid.vaccines.tweets %>%
filter(day_of_week == "Workday") %>%
unnest_tokens(word,text) %>%
filter(!word %in% stop_words$word) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment,sort=T)
weekend.tweets.props = covid.vaccines.tweets %>%
filter(day_of_week == "Weekend") %>%
unnest_tokens(word,text) %>%
filter(!word %in% stop_words$word) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment,sort=T)
x = c(weekend.tweets.props[1,]$n,
workday.tweets.props[1,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
outputNegatives = prop.test(x,n,alternative = "greater")
x = c(weekend.tweets.props[2,]$n,
workday.tweets.props[2,]$n)
n = c(weekend.tweets.props %>%
summarise(totalFreq=sum(n)) %>% .$totalFreq,
workday.tweets.props %>% summarise(totalFreq=sum(n)) %>% .$totalFreq)
outputPositives = prop.test(x,n,alternative = "less")
knitr::opts_chunk$set(echo = FALSE, warning=FALSE,
message=FALSE,cache = TRUE)
library(ggseas)
library(reshape2)
library(tidytext)
library(textdata)
library(dplyr)
library(ggmap)
library(devtools)
library(ggfortify)
library(sp)
library(rworldmap)
library(tidyverse)
library(gridExtra)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(stringi)
library(tm)
library(stringr)
library(quanteda.textstats)
covid.vaccines.tweets = read.csv("vaccination_all_tweets.csv",encoding="UTF-8")
covid.vaccines.tweets$text = as.character(covid.vaccines.tweets$text)
# using a lexicon to filter by
covid.vaccines.tweets$day_of_week = ifelse(
weekdays(as.Date(covid.vaccines.tweets$date,format="%Y-%m-%d"))
%in% c("Saturday","Sunday"),
"Weekend",
"Workday")
getPosNegatives= function(dayofweek, verified) {
covid.vaccines.tweets %>%
filter(day_of_week == dayofweek & user_verified == verified) %>%
unnest_tokens(word,text) %>%
filter(!word %in% stop_words$word) %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=T) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("firebrick", "forestgreen"),
max.words = 50,
random.order = F,
main=dayofweek)
title(paste(dayofweek,"and",
ifelse(verified=="True",
"Verified",
"Not Verified")),
line = -1.5)
}
vaccine.tweets = read.csv("vaccination_all_tweets.csv")
vaccine.tweets = read.csv("vaccination_all_tweets.csv")
validLocs = vaccine.tweets[grep("^[A-Za-z]+, [A-Za-z]+$",vaccine.tweets$user_location),]
validLocs = vaccine.tweets[grep("^[A-Za-z]+, [A-Za-z]+$",vaccine.tweets$user_location),]
cities.df = read.csv("vaccinesCity.csv")
missingCityLatLon = read.csv("missingCitiesVaccine.csv")
names(missingCityLatLon)[2] = "place"
missingCityLatLon$place = as.character(missingCityLatLon$place)
cities.df$place = as.character(cities.df$place)
cities.df = select(cities.df,c("place","lat","lon"))
missingCityLatLon = select(missingCityLatLon,c("place","lat","lon"))
transpose.apply.df = t(apply(cities.df,1,
function(row){
latitude = as.numeric(row[["lat"]])
placeName = as.character(row[["place"]])
if (is.na(latitude)) {
row.missing = missingCityLatLon[which(missingCityLatLon$place ==
placeName)[1],]
return(list(place=placeName,
lat=row.missing$lat,
lon=row.missing$lon
))
} else {
return(row)
}
}))
cities.df.new = data.frame(
place=as.character(transpose.apply.df[,1]),
lat=as.numeric(transpose.apply.df[,2]),
lon=as.numeric(transpose.apply.df[,3]))
cities.df.new = data.frame(
place=as.character(transpose.apply.df[,1]),
lat=as.numeric(transpose.apply.df[,2]),
lon=as.numeric(transpose.apply.df[,3]))
cities.df.new$place = as.character(cities.df.new$place)
validLocs$lat = cities.df.new$lat
validLocs$lon = cities.df.new$lon
cities.df$place = as.character(cities.df$place)
cities.df = select(cities.df,c("place","lat","lon"))
missingCityLatLon = select(missingCityLatLon,c("place","lat","lon"))
transpose.apply.df = t(apply(cities.df,1,
function(row){
latitude = as.numeric(row[["lat"]])
placeName = as.character(row[["place"]])
if (is.na(latitude)) {
row.missing = missingCityLatLon[which(missingCityLatLon$place ==
placeName)[1],]
return(list(place=placeName,
lat=row.missing$lat,
lon=row.missing$lon
))
} else {
return(row)
}
}))
cities.df.new = data.frame(
place=as.character(transpose.apply.df[,1]),
lat=as.numeric(transpose.apply.df[,2]),
lon=as.numeric(transpose.apply.df[,3]))
cities.df.new$place = as.character(cities.df.new$place)
validLocs$lat = cities.df.new$lat
validLocs$lon = cities.df.new$lon
world = c(left=-170,
bottom=-70,
right=170,
top=70)
missingCityLatLon$place = as.character(missingCityLatLon$place)
cities.df$place = as.character(cities.df$place)
cities.df = select(cities.df,c("place","lat","lon"))
missingCityLatLon = select(missingCityLatLon,c("place","lat","lon"))
transpose.apply.df = t(apply(cities.df,1,
function(row){
latitude = as.numeric(row[["lat"]])
placeName = as.character(row[["place"]])
if (is.na(latitude)) {
row.missing = missingCityLatLon[which(missingCityLatLon$place ==
placeName)[1],]
return(list(place=placeName,
lat=row.missing$lat,
lon=row.missing$lon
))
} else {
return(row)
}
}))
cities.df.new = data.frame(
place=as.character(transpose.apply.df[,1]),
lat=as.numeric(transpose.apply.df[,2]),
lon=as.numeric(transpose.apply.df[,3]))
cities.df.new$place = as.character(cities.df.new$place)
validLocs$lat = cities.df.new$lat
validLocs$lon = cities.df.new$lon
world = c(left=-170,
bottom=-70,
right=170,
top=70)
map = get_stamenmap(world,
zoom = 2,
maptype = "toner-lite")
nrow(validLocs)
world = c(left=-170,
bottom=-70,
right=170,
top=70)
map = get_stamenmap(world,
zoom = 2,
maptype = "toner-lite")
map = get_stamenmap(world,
zoom = 2,
maptype = "toner-lite")
validLocs.avg.followers = validLocs %>%
group_by(lat,lon) %>%
summarize(avg_followers=mean(user_followers))
validLocs$hashTagList = as.character(validLocs$hashtags)
validLocs$tagList = apply(validLocs,1,
function(row){
lstStr = row[["hashTagList"]]
items = gsub(" ", "",
strsplit(substr(lstStr,
2,
nchar(lstStr)-1),
","))[[1]]
return(tolower(items))
})
allTags = unlist(validLocs$tagList)
allTags = unlist(validLocs$tagList)
top8Tags = names(head(sort(table(allTags[allTags != "character(0)"]),
decreasing = T),8))
top8Tags
subsetRows = filter(validLocs, tagList %in% top8Tags)
nrow(subsetRows)
heart = read.csv("../data/heart.csv")
heart = read.csv("../data/heart.csv")
heart = read.csv("../data/heart/heart.csv")
heart = read.csv("../../data/heart/heart.csv")
head(heart)
shiny::runApp('Desktop/2022Spring/66501/visualize_periodic_traffic')
runApp('Desktop/2022Spring/66501/visualize_periodic_traffic')
?read_csv
?read_csv
runApp('Desktop/2022Spring/66501/visualize_periodic_traffic')
runApp('Desktop/2022Spring/66501/visualize_periodic_traffic')
runApp('Desktop/2022Spring/66501/visualize_periodic_traffic')
data$Date
data
data <- read_csv("../0data/A414.csv")
data <- read_csv("./0data/A414.csv")
setwd(".")
ls
data <- read_csv("./0data/A414.csv")
setwd("/Users/dylanc_home/Desktop/2022Spring/66501/")
data <- read_csv("./0data/A414.csv")
runApp('visualize_periodic_traffic')
data <- read_csv("./0data/A414.csv")
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
data
data["Date"]
data["Date"]
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
data <- read_csv("./data/A414.csv")
data["Date"] = as.Date(data["Date"],format="%m/%d/%Y %H:%M")
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
data[1,"Date"]
int(data[1,"Date"])
integer(data[1,"Date"])
data <- read_csv("./data/A414.csv")
dates <- data["Date"]
pwd
data <- read_csv("./data/A414.csv")
data <- read_csv("./data/A414.csv")
data <- read_csv("data/A414.csv")
data <- read_csv("./visualize_periodic_data/data/A414.csv")
data <- read_csv("visualize_periodic_data/data/A414.csv")
data <- read_csv("A414.csv")
data <- read_csv("./visualize_periodic_traffic/data/A414.csv")
data["Date"]
data["1,Date"]
data[1,"Date"]
as.Date(data[1,"Date"],format="%m/%d/%Y %H:%M")
data[1,"Date"]
data[1,"Date"][1]
unlist(data[1,"Date"])
unlist(data[1,"Date"])[1]
as.Date(data[1,"Date"],format="%m/%d/%Y %H:%M")
as.Date("1/2/2013 0:00",format="%m/%d/%Y %H:%M")
data[1,"Date"][[1]]
data[1,"Date"][[1]][1]
data[1,"Date"][[1]]
as.Date(data[1,"Date"][[1]],format="%m/%d/%Y %H:%M")
apply(data["Date"],function(date){as.Date(date[[1]],format="%m/%d/%Y %H:%M")})
apply(data["Date"],FUN=function(date){as.Date(date[[1]],format="%m/%d/%Y %H:%M")})
apply(data["Date"],FUN=function(date){as.Date(date[[1]],format="%m/%d/%Y %H:%M")},MARGIN=2)
apply(data["Date"],FUN=function(date){as.Date(date[[1]],format="%m/%d/%Y %H:%M")},MARGIN=1)
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
?ggplot
max(apply(data["Date"],FUN=function(date){as.Date(date[[1]],format="%m/%d/%Y %H:%M")},MARGIN=1))
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
data["Flow"]
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
runApp('visualize_periodic_traffic')
